
@article{agarwal_goal-constrained_2021,
	title = {Goal-constrained {Sparse} {Reinforcement} {Learning} for {End}-to-{End} {Driving}},
	url = {http://arxiv.org/abs/2103.09189},
	abstract = {Deep reinforcement Learning for end-to-end driving is limited by the need of complex reward engineering. Sparse rewards can circumvent this challenge but suffers from long training time and leads to sub-optimal policy. In this work, we explore full-control driving with only goal-constrained sparse reward and propose a curriculum learning approach for end-to-end driving using only navigation view maps that benefit from small virtual-to-real domain gap. To address the complexity of multiple driving policies, we learn concurrent individual policies selected at inference by a navigation system. We demonstrate the ability of our proposal to generalize on unseen road layout, and to drive significantly longer than in the training.},
	urldate = {2021-11-14},
	journal = {arXiv:2103.09189 [cs]},
	author = {Agarwal, Pranav and de Beaucorps, Pierre and de Charette, Raoul},
	month = jul,
	year = {2021},
	note = {arXiv: 2103.09189},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Robotics},
	file = {arXiv Fulltext PDF:files/3/Agarwal et al. - 2021 - Goal-constrained Sparse Reinforcement Learning for.pdf:application/pdf;arXiv.org Snapshot:files/4/2103.html:text/html},
}
